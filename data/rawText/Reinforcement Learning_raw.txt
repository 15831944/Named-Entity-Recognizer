 foru800 reinforcement learning inu800 psychology reinforcement inspired byu800 behaviorism behaviorist psychology  reinforcement learning  isu800 anu800 area ofu800 machine learning inu800 computer science concerned with howu800 software agent software agents ought tou800 take  actions  inu800 anu800  environment  sou800 asu800 tou800 maximize some notion ofu800 cumulative  reward . theu800 problem dueu800 tou800 itsu800 generality isu800 studied inu800 many other disciplines such asu800 game theory control theory operations research information theory simulation-based optimization statistics andu800 genetic algorithm algorithms. inu800 theu800 operations research andu800 control literature theu800 field where reinforcement learning methods areu800 studied isu800 called  approximate dynamic programming . theu800 problem hasu800 been studied inu800 theu800 optimal control theory theory ofu800 optimal control though most studies there areu800 concerned with existence ofu800 optimal solutions andu800 their characterization andu800 notu800 with theu800 learning oru800 approximation aspects. inu800 economics andu800 game theory reinforcement learning mayu800 beu800 used tou800 explain howu800 equilibrium mayu800 arise under bounded rationality . inu800 machine learning theu800 environment isu800 typically formulated asu800 au800 markov decision process mdpu800 andu800 many reinforcement learning algorithms foru800 this context areu800 highly related tou800 dynamic programming techniques. theu800 main difference between theu800 classical techniques andu800 reinforcement learning algorithms isu800 that theu800 latter dou800 notu800 need knowledge about theu800 mdpu800 andu800 they target large mdps where exact methods become infeasible. reinforcement learning differs from standard supervised learning inu800 that correct input output pairs areu800 never presented noru800 subu800-optimal actions explicitly corrected. further there isu800 au800 focus onu800 onu800-line performance which involves finding au800 balance between exploration ofu800 uncharted territory andu800 exploitation ofu800 current knowledge . theu800 exploration vsu800. exploitation trade-offu800 inu800 reinforcement learning hasu800 been most thoroughly studied through theu800 multi-armed bandit problem andu800 inu800 finite mdps. introduction introduction introduction theu800 basic reinforcement learning model consists ofu800 # au800 setu800 ofu800 environment states su800 ; # au800 setu800 ofu800 actions au800 ; # rules ofu800 transitioning between states; # rules that determine theu800  scalar immediate reward  ofu800 au800 transition; andu800 # rules that describe what theu800 agent observes. theu800 rules areu800 often stochastic . theu800 observation typically involves theu800 scalar immediate reward associated with theu800 last transition. inu800 many works theu800 agent isu800 also assumed tou800 observe theu800 current environmental state inu800 which case weu800 talk about  full observability  whereas inu800 theu800 opposing case weu800 talk about  partial observability . sometimes theu800 setu800 ofu800 actions available tou800 theu800 agent isu800 restricted eu800u82egu800. youu800 cannot spend more money than what youu800 possess . au800 reinforcement learning agent interacts with itsu800 environment inu800 discrete time steps. atu800 each time tu800 theu800 agent receives anu800 observation o_tu800 which typically includes theu800 reward r_tu800 . itu800 then chooses anu800 action a_tu800 from theu800 setu800 ofu800 actions available which isu800 subsequently sent tou800 theu800 environment. theu800 environment moves tou800 au800 newu800 state s_u800 tu800 1u800 andu800 theu800 reward r_u800 tu800 1u800 associated with theu800  transition  s_tu800 a_tu800 s_u800 tu800 1u800 isu800 determined. theu800 goal ofu800 au800 reinforcement learning agent isu800 tou800 collect asu800 much reward asu800 possible. theu800 software agent agent canu800 choose anyu800 action asu800 au800 function ofu800 theu800 history andu800 itu800 canu800 even randomize itsu800 action selection. when theu800 agent agent'su800 performance isu800 compared tou800 that ofu800 anu800 agent which acts optimally from theu800 beginning theu800 difference inu800 performance gives rise tou800 theu800 notion ofu800  regret . note that inu800 order tou800 actu800 near optimally theu800 agent must reason about theu800 long term consequences ofu800 itsu800 actions inu800 order tou800 maximize myu800 future income iu800 hadu800 better gou800 tou800 school nowu800 although theu800 immediate monetary reward associated with this might beu800 negative. thus reinforcement learning isu800 particularly well suited tou800 problems which include au800 long-term versus short-term reward trade-offu800. itu800 hasu800 been applied successfully tou800 various problems including robot control elevator scheduling telecommunications backgammon andu800 checkers #refsutton1998 sutton andu800 barto 1998 chapter 11u800 . twou800 components make reinforcement learning powerful theu800 useu800 ofu800 samples tou800 optimize performance andu800 theu800 useu800 ofu800 function approximation tou800 deal with large environments. thanks tou800 these twou800 keyu800 components reinforcement learning canu800 beu800 used inu800 large environments inu800 anyu800 ofu800 theu800 following situations au800 model ofu800 theu800 environment isu800 known butu800 anu800 analytic solution isu800 notu800 available; only au800 simulation model ofu800 theu800 environment isu800 given theu800 subject ofu800 simulation-based optimization ; theu800 only wayu800 tou800 collect information about theu800 environment isu800 byu800 interacting with itu800. theu800 first twou800 ofu800 these problems could beu800 considered planning problems since some form ofu800 theu800 model isu800 available while theu800 last oneu800 could beu800 considered asu800 au800 genuine learning problem. however under au800 reinforcement learning methodology both planning problems would beu800 converted tou800 machine learning problems. exploration exploration exploration theu800 reinforcement learning problem asu800 described requires clever exploration mechanisms. randomly selecting actions without reference tou800 anu800 estimated probability distribution isu800 known tou800 give rise tou800 very poor performance. theu800 case ofu800 small finite mdps isu800 relatively well understood byu800 nowu800. however dueu800 tou800 theu800 lack ofu800 algorithms that would provably scale well with theu800 number ofu800 states oru800 scale tou800 problems with infinite state spaces inu800 practice people resort tou800 simple exploration methods. oneu800 such method isu800 epsilon -greedy when theu800 agent chooses theu800 action that itu800 believes hasu800 theu800 best long-term effect with probability 1u800- epsilon andu800 itu800 chooses anu800 action uniformly atu800 random otherwise. here 0u800 epsilon 1u800 isu800 au800 tuning parameter which isu800 sometimes changed either according tou800 au800 fixed schedule making theu800 agent explore less asu800 time goes byu800 oru800 adaptively based onu800 some heuristics #reftokic2010 tokic & palm 2011 . algorithms foru800 control learning algorithms foru800 control learning algorithms foru800 control learning even ifu800 theu800 issue ofu800 exploration isu800 disregarded andu800 even ifu800 theu800 state wasu800 observable which weu800 assume from nowu800 onu800 theu800 problem remains tou800 find outu800 which actions areu800 good based onu800 past experience. criterion ofu800 optimality criterion ofu800 optimality criterion ofu800 optimality foru800 simplicity assume foru800 au800 moment that theu800 problem studied isu800  episodic  anu800 episode ending when some  terminal state  isu800 reached. assume further that nou800 matter what course ofu800 actions theu800 agent takes termination isu800 inevitable. under some additional mild regularity conditions theu800 expectation ofu800 theu800 total reward isu800 then well-defined foru800  anyu800  policy andu800 anyu800 initial distribution over theu800 states. here au800 policy refers tou800 au800 mapping that assigns some probability distribution over theu800 actions tou800 allu800 possible histories. given au800 fixed initial distribution muu800 weu800 canu800 thus assign theu800 expected return rhou800 piu800 tou800 policy piu800 rhou800 piu800 eu800 ru800 piu800 where theu800 random variable ru800 denotes theu800  return  andu800 isu800 defined byu800 ru800 sum_ tu800 0u800 nu800-1u800 r_u800 tu800 1u800 where r_u800 tu800 1u800 isu800 theu800 reward received after theu800 tu800 -thu800 transition theu800 initial state isu800 sampled atu800 random from muu800 andu800 actions areu800 selected byu800 policy piu800 . here nu800 denotes theu800 random time when au800 terminal state isu800 reached iu800u82eeu800. theu800 time when theu800 episode terminates. inu800 theu800 case ofu800 nonu800-episodic problems theu800 return isu800 often  discounted  ru800 sum_ tu800 0u800 infty gamma tu800 r_u800 tu800 1u800 giving rise tou800 theu800 total expected discounted reward criterion. here 0u800 leu800 gamma leu800 1u800 isu800 theu800 sou800-called  discount-factor . since theu800 undiscounted return isu800 au800 special case ofu800 theu800 discounted return from nowu800 onu800 weu800 will assume discounting. although this looks innocent enough discounting isu800 inu800 fact problematic ifu800 oneu800 cares about online performance. this isu800 because discounting makes theu800 initial time steps more important. since au800 learning agent isu800 likely tou800 make mistakes during theu800 first fewu800 steps after itsu800 life starts nou800 uninformed learning algorithm canu800 achieve near-optimal performance under discounting even ifu800 theu800 class ofu800 environments isu800 restricted tou800 that ofu800 finite mdps. this does notu800 mean though that given enough time au800 learning agent cannot figure howu800 tou800 actu800 near-optimally ifu800 time wasu800 restarted. theu800 problem then isu800 tou800 specify anu800 algorithm that canu800 beu800 used tou800 find au800 policy with maximum expected return. from theu800 theory ofu800 mdps itu800 isu800 known that without loss ofu800 generality theu800 search canu800 beu800 restricted tou800 theu800 setu800 ofu800 theu800 sou800-called  stationary  policies. au800 policy isu800 called stationary ifu800 theu800 action-distribution returned byu800 itu800 depends only onu800 theu800 last state visited which isu800 part ofu800 theu800 observation history ofu800 theu800 agent byu800 ouru800 simplifying assumption . inu800 fact theu800 search canu800 beu800 further restricted tou800  deterministic  stationary policies. au800 deterministic stationary policy isu800 oneu800 which deterministically selects actions based onu800 theu800 current state. since anyu800 such policy canu800 beu800 identified with au800 mapping from theu800 setu800 ofu800 states tou800 theu800 setu800 ofu800 actions these policies canu800 beu800 identified with such mappings with nou800 loss ofu800 generality. brute force brute force brute force theu800 brute-force search brute force approach entails theu800 following twou800 steps # foru800 each possible policy sample returns while following itu800 # choose theu800 policy with theu800 largest expected return oneu800 problem with this isu800 that theu800 number ofu800 policies canu800 beu800 extremely large oru800 even infinite. another isu800 that variance ofu800 theu800 returns might beu800 large inu800 which case au800 large number ofu800 samples will beu800 required tou800 accurately estimate theu800 return ofu800 each policy. these problems canu800 beu800 ameliorated ifu800 weu800 assume some structure andu800 perhaps allow samples generated from oneu800 policy tou800 influence theu800 estimates made foru800 another. theu800 twou800 main approaches foru800 achieving this areu800 value function approaches value function estimation andu800 direct policy search . value function approaches value function approaches value function approaches value function approaches attempt tou800 find au800 policy that maximizes theu800 return byu800 maintaining au800 setu800 ofu800 estimates ofu800 expected returns foru800 some policy usually either theu800 current oru800 theu800 optimal oneu800 . these methods rely onu800 theu800 theory ofu800 mdps where optimality isu800 defined inu800 au800 sense which isu800 stronger than theu800 above oneu800 au800 policy isu800 called optimal ifu800 itu800 achieves theu800 best expected return from  anyu800  initial state iu800u82eeu800. initial distributions play nou800 role inu800 this definition . again oneu800 canu800 always find anu800 optimal policy amongst stationary policies. tou800 define optimality inu800 au800 formal manner define theu800 value ofu800 au800 policy piu800 byu800 vu800 piu800 su800 eu800 ru800 su800 piu800 where ru800 stands foru800 theu800 random return associated with following piu800 from theu800 initial state su800 . define vu800 su800 asu800 theu800 maximum possible value ofu800 vu800 piu800 su800 where piu800 isu800 allowed tou800 change vu800 su800 supu800 limits_ piu800 vu800 piu800 su800 . au800 policy which achieves these  optimal values  inu800  each  state isu800 called  optimal . clearly au800 policy optimal inu800 this strong sense isu800 also optimal inu800 theu800 sense that itu800 maximizes theu800 expected return rhou800 piu800 since rhou800 piu800 eu800 vu800 piu800 su800 where su800 isu800 au800 state randomly sampled from theu800 distribution muu800 . although state-values suffice tou800 define optimality itu800 will prove tou800 beu800 useful tou800 define action-values. given au800 state su800 anu800 action au800 andu800 au800 policy piu800 theu800 action-value ofu800 theu800 pair su800 au800 under piu800 isu800 defined byu800 qu800 piu800 su800 au800 eu800 ru800 su800 au800 piu800 where nowu800 ru800 stands foru800 theu800 random return associated with first taking action au800 inu800 state su800 andu800 following piu800 thereafter. itu800 isu800 well-known from theu800 theory ofu800 mdps that ifu800 someone gives usu800 qu800 foru800 anu800 optimal policy weu800 canu800 always choose optimal actions andu800 thus actu800 optimally byu800 simply choosing theu800 action with theu800 highest value atu800 each state. theu800  action-value function  ofu800 such anu800 optimal policy isu800 called theu800  optimal action-value function  andu800 isu800 denoted byu800 qu800 . inu800 summary theu800 knowledge ofu800 theu800 optimal action-value function  alone  suffices tou800 know howu800 tou800 actu800 optimally. assuming full knowledge ofu800 theu800 mdpu800 there areu800 twou800 basic approaches tou800 compute theu800 optimal action-value function value iteration andu800 policy iteration . both algorithms compute au800 sequence ofu800 functions q_ku800 ku800 0u800 1u800 2u800 ldots which converge tou800 qu800 . computing these functions involves computing expectations over theu800 whole state-space which isu800 impractical foru800 allu800 butu800 theu800 smallest finite mdps never mind theu800 case when theu800 mdpu800 isu800 unknown. inu800 reinforcement learning methods theu800 expectations areu800 approximated byu800 averaging over samples andu800 oneu800 uses function approximation techniques tou800 cope with theu800 need tou800 represent value functions over large state-action spaces. monte carlo methods monte carlo methods monte carlo methods theu800 simplest monte carlo sampling monte carlo methods canu800 beu800 used inu800 anu800 algorithm that mimics policy iteration. policy iteration consists ofu800 twou800 steps  policy evaluation  andu800  policy improvement . theu800 monte carlo methods areu800 used inu800 theu800 policy evaluation step. inu800 this step given au800 stationary deterministic policy piu800 theu800 goal isu800 tou800 compute theu800 function values qu800 piu800 su800 au800 oru800 au800 good approximation tou800 them foru800 allu800 state-action pairs su800 au800 . assume foru800 simplicity that theu800 mdpu800 isu800 finite andu800 inu800 fact au800 table representing theu800 action-values fits into theu800 memory. further assume that theu800 problem isu800 episodic andu800 after each episode au800 newu800 oneu800 starts from some random initial state. then theu800 estimate ofu800 theu800 value ofu800 au800 given state-action pair su800 au800 canu800 beu800 computed byu800 simply averaging theu800 sampled returns which originated from su800 au800 over time. given enough time this procedure canu800 thus construct au800 precise estimate qu800 ofu800 theu800 action-value function qu800 piu800 . this finishes theu800 description ofu800 theu800 policy evaluation step. inu800 theu800 policy improvement step asu800 itu800 isu800 done inu800 theu800 standard policy iteration algorithm theu800 next policy isu800 obtained byu800 computing au800  greedy  policy with respect tou800 qu800 given au800 state su800 this newu800 policy returns anu800 action that maximizes qu800 su800 cdot . inu800 practice oneu800 often avoids computing andu800 storing theu800 newu800 policy butu800 uses lazy evaluation tou800 defer theu800 computation ofu800 theu800 maximizing actions tou800 when they areu800 actually needed. au800 fewu800 problems with this procedure areu800 asu800 follows theu800 procedure mayu800 waste toou800 much time onu800 evaluating au800 suboptimal policy; itu800 uses samples inefficiently inu800 that au800 long trajectory isu800 used tou800 improve theu800 estimate only ofu800 theu800  single  state-action pair that started theu800 trajectory; when theu800 returns along theu800 trajectories have  high variance  convergence will beu800 slow; itu800 works inu800  episodic problems only ; itu800 works inu800  small finite mdps only . temporal difference methods temporal difference methods temporal difference methods theu800 first issue isu800 easily corrected byu800 allowing theu800 procedure tou800 change theu800 policy atu800 allu800 oru800 atu800 some states before theu800 values settle. however good this sounds this mayu800 beu800 dangerous asu800 this might prevent convergence. still most current algorithms implement this idea giving rise tou800 theu800 class ofu800  generalized policy iteration  algorithm. weu800 note inu800 passing that actor critic methods belong tou800 this category. theu800 second issue canu800 beu800 corrected within theu800 algorithm byu800 allowing trajectories tou800 contribute tou800 anyu800 state-action pair inu800 them. this mayu800 also help tou800 some extent with theu800 third problem although au800 better solution when returns have high variance isu800 tou800 useu800 #refsutton84 #refsutton88 sutton 'su800 temporal difference tdu800 methods which areu800 based onu800 theu800 recursive bellman equations bellman equation . note that theu800 computation inu800 tdu800 methods canu800 beu800 incremental when after each transition theu800 memory isu800 changed andu800 theu800 transition isu800 thrown away oru800 batch when theu800 transitions areu800 collected andu800 then theu800 estimates areu800 computed once based onu800 au800 large number ofu800 transitions . batch methods au800 prime example ofu800 which isu800 theu800 least-squares temporal difference method dueu800 tou800 #refbradtke1996 bradtke andu800 barto 1996 mayu800 useu800 theu800 information inu800 theu800 samples better whereas incremental methods areu800 theu800 only choice when batch methods become infeasible dueu800 tou800 their high computational oru800 memory complexity. inu800 addition there exist methods that tryu800 tou800 unify theu800 advantages ofu800 theu800 twou800 approaches. methods based onu800 temporal differences also overcome theu800 second butu800 last issue. inu800 order tou800 address theu800 last issue mentioned inu800 theu800 previous section  function approximation methods  areu800 used. inu800  linear function approximation  oneu800 starts with au800 mapping phiu800 that assigns au800 finite dimensional vector tou800 each state-action pair. then theu800 action values ofu800 au800 state-action pair su800 au800 areu800 obtained byu800 linearly combining theu800 components ofu800 phiu800 su800 au800 with some  weights  theta qu800 su800 au800 sumu800 limits_ iu800 1u800 du800 theta_i phi_i su800 au800 . theu800 algorithms then adjust theu800 weights instead ofu800 adjusting theu800 values associated with theu800 individual state-action pairs. however linear function approximation isu800 notu800 theu800 only choice. more recently methods based onu800 ideas from nonparametric statistics which canu800 beu800 seen tou800 construct their ownu800 features have been explored. sou800 faru800 theu800 discussion wasu800 restricted tou800 howu800 policy iteration canu800 beu800 used asu800 au800 basis ofu800 theu800 designing reinforcement learning algorithms. equally importantly value iteration canu800 also beu800 used asu800 au800 starting point giving rise tou800 theu800 qu800-learning algorithm #refwatkins1989 watkins 1989 andu800 itsu800 many variants. theu800 problem with methods that useu800 action-values isu800 that they mayu800 need highly precise estimates ofu800 theu800 competing action values which canu800 beu800 hard tou800 obtain when theu800 returns areu800 noisy. though this problem isu800 mitigated tou800 some extent byu800 temporal difference methods andu800 ifu800 oneu800 uses theu800 sou800-called compatible function approximation method more work remains tou800 beu800 done tou800 increase generality andu800 efficiency. another problem specific tou800 temporal difference methods comes from their reliance onu800 theu800 recursive bellman equation. most temporal difference methods have au800 sou800-called lambda parameter 0u800 leu800 lambda leu800 1u800 that allows oneu800 tou800 continuously interpolate between monte-carlo methods which dou800 notu800 rely onu800 theu800 bellman equations andu800 theu800 basic temporal difference methods which rely entirely onu800 theu800 bellman equations which canu800 thus beu800 effective inu800 palliating this issue. direct policy search direct policy search direct policy search anu800 alternative method tou800 find au800 good policy isu800 tou800 search directly inu800 some subset ofu800 theu800 policy space inu800 which case theu800 problem becomes anu800 instance ofu800 stochastic optimization . theu800 twou800 approaches available areu800 gradient-based andu800 gradient-free methods. gradient-based methods giving rise tou800 theu800 sou800-called  policy gradient methods  start with au800 mapping from au800 finite dimensional parameter space tou800 theu800 space ofu800 policies given theu800 parameter vector theta letu800 pi_u800 theta denote theu800 policy associated tou800 theta . define theu800 performance function byu800 rhou800 theta rhou800 pi_u800 theta . under mild conditions this function will beu800 differentiable asu800 au800 function ofu800 theu800 parameter vector theta . ifu800 theu800 gradient ofu800 rhou800 wasu800 known oneu800 could useu800 gradient descent gradient ascent . since anu800 analytic expression foru800 theu800 gradient isu800 notu800 available oneu800 must rely onu800 au800 noisy estimate. such anu800 estimate canu800 beu800 constructed inu800 many ways giving rise tou800 algorithms like #refwilliams1987 williams reinforce method which isu800 also known asu800 theu800 likelihood ratio method inu800 theu800 simulation-based optimization literature . policy gradient methods have received au800 lotu800 ofu800 attention inu800 theu800 last couple ofu800 years eu800u82egu800. #refpeters2003 peters etu800 alu800. 2003 butu800 they remain anu800 active field. theu800 issue with many ofu800 these methods isu800 that they mayu800 getu800 stuck inu800 local optima asu800 they areu800 based onu800 local search optimization local search . au800 large class ofu800 methods avoids relying onu800 gradient information. these include simulated annealing cross-entropy method cross-entropy search oru800 methods ofu800 evolutionary computation . many gradient-free methods canu800 achieve inu800 theory andu800 inu800 theu800 limit au800 global optimum. inu800 au800 number ofu800 cases they have indeed demonstrated remarkable performance. theu800 issue with policy search methods isu800 that they mayu800 converge slowly ifu800 theu800 information based onu800 which they actu800 isu800 noisy. foru800 example this happens when inu800 episodic problems theu800 trajectories areu800 long andu800 theu800 variance ofu800 theu800 returns isu800 large. asu800 argued beforehand value-function based methods that rely onu800 temporal differences might help inu800 this case. inu800 recent years several actor-critic algorithms have been proposed following this idea andu800 were demonstrated tou800 perform well inu800 various problems. theory theory theory theu800 theory foru800 small finite mdps isu800 quite mature. both theu800 asymptotic andu800 finite-sample behavior ofu800 most algorithms isu800 well-understood. asu800 mentioned beforehand algorithms with provably good online performance addressing theu800 exploration issue areu800 known. theu800 theory ofu800 large mdps needs more work. efficient exploration isu800 largely untouched except foru800 theu800 case ofu800 bandit problems . although finite-time performance bounds appeared foru800 many algorithms inu800 theu800 recent years these bounds areu800 expected tou800 beu800 rather loose andu800 thus more work isu800 needed tou800 better understand theu800 relative advantages asu800 well asu800 theu800 limitations ofu800 these algorithms. foru800 incremental algorithm asymptotic convergence issues have been settled. recently newu800 incremental temporal-difference-based algorithms have appeared which converge under au800 much wider setu800 ofu800 conditions than wasu800 previously possible foru800 example when used with arbitrary smooth function approximation . current research current research current research current research topics include adaptive methods which work with fewer oru800 nou800 parameters under au800 large number ofu800 conditions addressing theu800 exploration problem inu800 large mdps large-scale empirical evaluations learning andu800 acting under partially observable markov decision process partial information eu800u82egu800. using predictive state representation modular andu800 hierarchical reinforcement learning improving existing value-function andu800 policy search methods algorithms that work well with large oru800 continuous action spaces transfer learning lifelong learning efficient sample-based planning eu800u82egu800. based onu800 monte-carlo tree search . multiagent oru800 distributed reinforcement learning isu800 also au800 topic ofu800 interest inu800 current research. there isu800 also au800 growing interest inu800 real life applications ofu800 reinforcement learning. successes ofu800 reinforcement learning areu800 collected onu800 here andu800 here . reinforcement learning algorithms such asu800 tdu800 learning areu800 also being investigated asu800 au800 model foru800 dopamine -based learning inu800 theu800 brain. inu800 this model theu800 dopaminergic projections from theu800 substantia nigra tou800 theu800 basal ganglia function asu800 theu800 prediction error. reinforcement learning hasu800 also been used asu800 au800 part ofu800 theu800 model foru800 human skill learning especially inu800 relation tou800 theu800 interaction between implicit andu800 explicit learning inu800 skill acquisition theu800 first publication onu800 this application wasu800 inu800 1995-1996 andu800 there have been many follow-upu800 studies . seeu800 foru800 further details ofu800 these research areas above. literature literature literature conferences journals conferences journals conferences journals most reinforcement learning papers areu800 published atu800 theu800 major machine learning andu800 aiu800 conferences icml nips aaai ijcai uaiu800 aiu800 andu800 statistics andu800 journals jair jmlr machine learning journal . some theory papers areu800 published atu800 colt andu800 altu800. however many papers appear inu800 robotics conferences iros icra andu800 theu800 agent conference aamas. operations researchers publish their papers atu800 theu800 informs conference andu800 foru800 example inu800 theu800 operation research andu800 theu800 mathematics ofu800 operations research journals. control researchers publish their papers atu800 theu800 cdcu800 andu800 accu800 conferences oru800 eu800u82egu800. inu800 theu800 journals ieee transactions onu800 automatic control oru800 automatica although applied works tend tou800 beu800 published inu800 more specialized journals. theu800 winter simulation conference also publishes many relevant papers. other than this papers also published inu800 theu800 major conferences ofu800 theu800 neural networks fuzzy andu800 evolutionary computation communities. theu800 annual ieee symposium titled approximate dynamic programming andu800 reinforcement learning adprl andu800 theu800 biannual european workshop onu800 reinforcement learning ewrl areu800 twou800 regularly held meetings where rlu800 researchers meet. seeu800 also seeu800 also seeu800 also marcus hutter hutter'su800 universal artificial intelligence also called aixi temporal difference learning qu800-learning sarsa fictitious play learning classifier system optimal control dynamic treatment regimes error-driven learning implementations implementations implementations rlu800-glue provides au800 standard interface that allows youu800 tou800 connect agents environments andu800 experiment programs together even ifu800 they areu800 written inu800 different languages. maja machine learning framework theu800 maja machine learning framework mmlf isu800 au800 general framework foru800 problems inu800 theu800 domain ofu800 reinforcement learning rlu800 written inu800 python. software tools foru800 reinforcement learning matlab andu800 python pybrain python teachingbox isu800 au800 java reinforcement learning framework supporting many features like rbfu800 networks gradient descent learning methods ... cu800 implementation foru800 some well known reinforcement learning algorithms with source. orange software orange au800 free data mining software suite module orngreinforcement policy gradient toolbox provides au800 package foru800 learning about policy gradient approaches. references references references cite thesis last sutton first richard su800. authorlink richard su800. sutton degree phdu800 title temporal credit assignment inu800 reinforcement learning year 1984 school university ofu800 massachusetts amherst mau800 urlu800 cite conference last williams first ronald ju800. authorlink ronald ju800. williams title au800 class ofu800 gradient-estimating algorithms foru800 reinforcement learning inu800 neural networks booktitle proceedings ofu800 theu800 ieee first international conference onu800 neural networks year 1987 urlu800 cite journal doiu800 10u800u82e1007 bf00115009 last sutton first richard su800. authorlink richard su800. sutton title learning tou800 predict byu800 theu800 method ofu800 temporal differences journal machine learning volume 3u800 pages 9u800&ndash;44u800 publisher springer year 1988 urlu800 cite thesis last watkins first christopher ju800u82ecu800u82ehu800. authorlink christopher ju800u82ecu800u82ehu800. watkins degree phdu800 title learning from delayed rewards year 1989 school kingu8e28099s college cambridge uku800 urlu800 cite journal doiu800 10u800u82e1023 au800 1018056104778 last bradtke first steven ju800. authorlink steven ju800. bradtke coauthors andrew gu800. barto title learning tou800 predict byu800 theu800 method ofu800 temporal differences journal machine learning volume 22u800 pages 33u800&ndash;57u800 publisher springer year 1996 urlu800 cite book last bertsekas first dimitri pu800. authorlink dimitri pu800. bertsekas coauthors john tsitsiklis title neuro-dynamic programming publisher athena scientific year 1996 location nashua nhu800 isbn 1u800-886529-10u800-8u800 urlu800 cite journal last kaelbling first leslie pu800. authorlink leslie pu800. kaelbling coauthors michael lu800. littman ; andrew wu800. moore title reinforcement learning au800 survey journal journal ofu800 artificial intelligence research volume 4u800 pages 237u800&ndash;285u800 publisher year 1996 urlu800 cite book last sutton first richard su800. authorlink richard su800. sutton coauthors barto andrew gu800. title reinforcement learning anu800 introduction publisher mitu800 press year 1998 isbn 0u800-262u800-19398-1u800 pages urlu800 cite conference last peters first janu800 authorlink janu800 peters researcher coauthors sethu vijayakumar ; stefan schaal title reinforcement learning foru800 humanoid robotics booktitle ieee-rasu800 international conference onu800 humanoid robots year 2003 urlu800 cite book last powell first warren title approximate dynamic programming solving theu800 curses ofu800 dimensionality year 2007 publisher wiley-interscience isbn 0u800-470u800-17155-3u800 urlu800 cite journal last auer first peter authorlink peter auer coauthors thomas jaksch ; ronald ortner title near-optimal regret bounds foru800 reinforcement learning journal journal ofu800 machine learning research volume 11u800 pages 1563&ndash;1600 publisher year 2010 urlu800 cite conference last szita first istvan authorlink istvan szita title model-based reinforcement learning with nearly tight exploration complexity bounds coauthors csaba szepesvari year 2010 publisher omnipress booktitle icml 2010 pages 1031u8e280931038 urlu800 cite book last bertsekas first dimitri pu800. authorlink dimitri pu800. bertsekas title dynamic programming andu800 optimal control year 2010 month august edition 3u800 volume iiu800 chapter chapter 6u800 online approximate dynamic programming urlu800 cite book last busoniu first lucian authorlink lucian busoniu coauthors robert babuska ; bart deu800 schutter ; damien ernst title reinforcement learning andu800 dynamic programming using function approximators publisher taylor & francis crcu800 press year 2010 isbn 978u800-1u800-4398-2108-4u800 pages urlu800 cite book publisher springer berlin heidelberg volume 7006 pages 335u8e28093346 last tokic first michel authorlink michel tokic coauthors gu8c3bcnther palm ; title kiu800 2011 advances inu800 artificial intelligence chapter value-difference based exploration adaptive control between epsilon-greedy andu800 softmax series lecture notes inu800 computer science urlu800 year 2011 external links external links external links reinforcement learning repository reinforcement learning andu800 artificial intelligence sutton sutton'su800 labu800 atu800 theu800 university ofu800 alberta autonomous learning laboratory barto barto'su800 labu800 atu800 theu800 university ofu800 massachusetts amherst rlu800-glue software tools foru800 reinforcement learning matlab andu800 python theu800 uofa reinforcement learning library texts theu800 reinforcement learning toolbox from theu800 graz university ofu800 technology hybrid reinforcement learning piqle au800 generic java platform foru800 reinforcement learning au800 short introduction tou800 some reinforcement learning algorithms reinforcement learning applied tou800 ticu800-tacu800-toeu800 game scholarpedia reinforcement learning scholarpedia temporal difference learning stanford reinforcement learning course real-world reinforcement learning experiments atu800 delft university ofu800 technology reinforcement learning tools foru800 matlab stanford university andrew ngu800 lecture onu800 reinforcement learning relative entropy policy search category markov models category machine learning algorithms category belief revision 